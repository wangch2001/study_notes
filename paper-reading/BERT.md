# BERT

BERT 并不是一个具有极大创新的算法，更多的是一个集大成者，把 BERT 之前各个语言模型的优点集于一身，并作出了适当的改进，而拥有了如今无与伦比的能力。

BERT 既然是集大成者，那么集了哪些大成呢？

1. BERT 作为一个预训练语言模型，它的预训练思想借鉴了图像领域中的**预训练**的思想；
2. 作者说的是**借鉴了完形填空任务的思想（双向编码）**，但感觉应该也借鉴了 Word2Vec 的 CBOW 的思想，因为两者本质上是相同的；
3. 没有使用传统的类 RNN 模型作为特征提取器，而是使用了**最近火热的 Transformer 作为特征提取器，而 Transformer 又使用了 Attention 机制作为特征提取器，更是火上浇油**；
4. 真要说创新，也许就是在 CBOW 的思想之上，添加了语言掩码模型（MLM），减少了训练阶段和推理阶段（测试阶段）的不匹配，避免了过拟合；
5. 由于单词预测粒度的训练到不了句子关系这个层级，为了**学会捕捉句子之间的语义联系**，BERT 采用了下句预测（NSP ）作为无监督预训练的一部分，这也算是一个小小的创新吧。

# Word2Vec模型

利用上下文的单词，来预测中间的单词，对于一个单词的解释，利用上下文信息作出的解释会更合理

Word2Vec 还提供了 2 种训练方法：

1. 第一种叫 CBOW，**核心思想是从一个句子里面把一个词抠掉**，用这个词的上文和下文去预测被抠掉的这个词；
2. 第二种叫做 Skip-gram，和 CBOW 正好反过来，输入某个单词，要求网络预测它的上下文单词。

<img src="./assets/image-20250513103414976.png" alt="image-20250513103414976" style="zoom:67%;" />

# 1.BERT：公认的里程碑

- BERT 的意义在于：从大量无标记数据集中训练得到的深度模型，可以显著提高各项自然语言处理任务的准确率。
- 近年来优秀预训练语言模型的集大成者：参考了 ELMO 模型的双向编码思想、借鉴了 GPT 用 Transformer 作为特征提取器的思路、采用了 word2vec 所使用的 CBOW 方法
- BERT 和 GPT 之间的区别：
  - GPT：**GPT 使用 Transformer Decoder 作为特征提取器、具有良好的文本生成能力**，然而当前词的语义只能由其前序词决定，并且在语义理解上不足
  - BERT：使用了 Transformer Encoder 作为特征提取器，并使用了与其配套的掩码训练方法。**虽然使用双向编码让 BERT 不再具有文本生成能力，但是 BERT 的语义信息提取能力更强**
- 单向编码和双向编码的差异，以该句话举例 “今天天气很{}，我们不得不取消户外运动”，分别从单向编码和双向编码的角度去考虑 {} 中应该填什么词：
  - 单向编码：单向编码只会考虑 “今天天气很”，以人类的经验，大概率会从 “好”、“不错”、“差”、“糟糕” 这几个词中选择，这些词可以被划为截然不同的两类
  - 双向编码：**双向编码会同时考虑上下文的信息**，即除了会考虑 “今天天气很” 这五个字，还会考虑 “我们不得不取消户外运动” 来帮助模型判断，则大概率会从 “差”、“糟糕” 这一类词中选择

# 2.BERT的结构

![image-20250513104050066](./assets/image-20250513104050066.png)

- ELMo 使用自左向右编码和自右向左编码的两个 LSTM 网络，分别以 $P(w_i|w_1,\cdots,w_{i-1})$ 和 $P(w_i|w_{i+1},\cdots,w_n)$ 为目标函数独立训练，**将训练得到的特征向量以拼接的形式实现双向编码，本质上还是单向编码，只不过是两个方向上的单向编码的拼接而成的双向编码**。
- GPT 使用 Transformer Decoder 作为 Transformer Block，以 $P(w_i|w_1,\cdots,w_{i-1})$ 为目标函数进行训练，**用 Transformer Block 取代 LSTM 作为特征提取器，实现了单向编码，是一个标准的预训练语言模型，即使用 Fine-Tuning 模式解决下游任务。**
- BERT 也是一个标准的预训练语言模型，**它以 $P(w_i|w_1,\cdots,w_{i-1},w_{i+1},\cdots,w_n)$ 为目标函数进行训练，BERT 使用的编码器属于双向编码器**。

























