# 10. 注意力机制

 Bahdanau注意力是深度学习中的具有突破性价值的注意力模型，它双向对齐并且可以微分。

最后将描述仅仅基于注意力机制的*Transformer*架构， 该架构中使用了*多头注意力*（multi-head attention） 和*自注意力*（self-attention）。

# 10.1. 注意力提示

## 10.1.1. 生物学中的注意力提示

## 10.1.2. 查询、键和值

自主性的与非自主性的注意力提示解释了人类的注意力的方式

“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。

在注意力机制的背景下，自主性提示被称为*查询*（query）。

给定任何查询，注意力机制通过*注意力汇聚*（attention pooling） 将选择引导至*感官输入*（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为*值*（value）。 更通俗的解释，每个值都与一个*键*（key）配对， 这可以想象为感官输入的非自主提示。更通俗的解释，每个值都与一个*键*（key）配对， 这可以想象为感官输入的非自主提示。 如图所示，可以通过设计注意力汇聚的方式， 便于给定的查询（自主性提示）与键（非自主性提示）进行匹配， 这将引导得出最匹配的值（感官输入）。

![image-20250505144531954](./assets/image-20250505144531954.png)

## 10.1.3. 注意力的可视化



## 10.1.4. 小结

- 人类的注意力是有限的、有价值和稀缺的资源。
- 受试者使用非自主性和自主性提示有选择性地引导注意力。前者基于突出性，后者则依赖于意识。
- 注意力机制与全连接层或者汇聚层的区别源于增加的自主提示。
- 由于包含了自主性提示，注意力机制与全连接的层或汇聚层不同。
- 注意力机制通过注意力汇聚使选择偏向于值（感官输入），其中包含查询（自主性提示）和键（非自主性提示）。键和值是成对的。
- 可视化查询和键之间的注意力权重是可行的。

# 10.2. 注意力汇聚：Nadaraya-Watson 核回归

1964年提出的Nadaraya-Watson核回归模型 是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习。

## 10.2.1. 生成数据集

## 10.2.2. 平均汇聚

## 10.2.3. 非参数注意力汇聚

Nadaraya-Watson核回归是一个非参数模型。

因此由观察可知“查询-键”对越接近， 注意力汇聚的注意力权重就越高。

## 10.2.4. 带参数注意力汇聚

非参数的Nadaraya-Watson核回归具有*一致性*（consistency）的优点： 如果有足够的数据，此模型会收敛到最优结果

例如，与 [(10.2.6)](https://zh.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html#equation-eq-nadaraya-watson-gaussian)略有不同， 在下面的查询x和键xi之间的距离乘以可学习参数w：

![image-20250505145908255](./assets/image-20250505145908255.png)

### 10.2.4.1. 批量矩阵乘法

## 10.2.5. 小结

- Nadaraya-Watson核回归是具有注意力机制的机器学习范例。
- Nadaraya-Watson核回归的注意力汇聚是对训练数据中输出的加权平均。从注意力的角度来看，分配给每个值的注意力权重取决于将值所对应的键和查询作为输入的函数。
- 注意力汇聚可以分为非参数型和带参数型。

# 10.3. 注意力评分函数

高斯核指数部分可以视为*注意力评分函数*（attention scoring function）， 简称*评分函数*（scoring function）， 然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。

 [图10.3.1](https://zh.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html#fig-attention-output)说明了 如何将注意力汇聚的输出计算成为值的加权和， 其中a表示注意力评分函数。 由于注意力权重是概率分布， 因此加权和其本质上是加权平均值。

![image-20250505150826136](./assets/image-20250505150826136.png)

用数学语言描述，假设有一个查询 q∈Rq和 m个“键－值”对 (k1,v1),…,(km,vm)， 其中ki∈Rk，vi∈Rv。 注意力汇聚函数f就被表示成值的加权和：

![image-20250505150937795](./assets/image-20250505150937795.png)

其中查询q和键ki的注意力权重（标量） 是通过注意力评分函数a将两个向量映射成标量， 再经过softmax运算得到的：

![image-20250505150945519](./assets/image-20250505150945519.png)

正如上图所示，选择不同的注意力评分函数a会导致不同的注意力汇聚操作。 本节将介绍两个流行的评分函数，稍后将用他们来实现更复杂的注意力机制。

## 10.3.1. 掩蔽softmax操作

 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。为了仅将有意义的词元作为值来获取注意力汇聚， 可以指定一个有效序列长度（即词元的个数）， 以便在计算softmax时过滤掉超出指定范围的位置。

## 10.3.2. 加性注意力

一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。给定查询q∈Rq和 键k∈Rk， *加性注意力*（additive attention）的评分函数为

![image-20250505152718069](./assets/image-20250505152718069.png)

## 10.3.3. 缩放点积注意力

使用点积可以得到计算效率更高的评分函数， 但是点积操作要求查询和键具有相同的长度d。 假设查询和键的所有元素都是独立的随机变量， 并且都满足零均值和单位方差， 那么两个向量的点积的均值为0，方差为d。 为确保无论向量长度如何， 点积的方差在不考虑向量长度的情况下仍然是1， 我们再将点积除以d， 则*缩放点积注意力*（scaled dot-product attention）评分函数为：

![image-20250505152853779](./assets/image-20250505152853779.png)

## 10.3.4. 小结

- 将注意力汇聚的输出计算可以作为值的加权平均，选择不同的注意力评分函数会带来不同的注意力汇聚操作。
- 当查询和键是不同长度的矢量时，可以使用可加性注意力评分函数。当它们的长度相同时，使用缩放的“点－积”注意力评分函数的计算效率更高。

# 10.4. Bahdanau 注意力

 Bahdanau等人提出了一个没有严格单向对齐限制的 可微注意力模型 ([Bahdanau *et al.*, 2014](https://zh.d2l.ai/chapter_references/zreferences.html#id6))。在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。这是通过将上下文变量视为注意力集中的输出来实现的。

## 10.4.1. 模型

![image-20250505153651257](./assets/image-20250505153651257.png)

其中，时间步t′−1时的解码器隐状态st′−1是查询， 编码器隐状态ht既是键，也是值， 注意力权重α是使用 [(10.3.2)](https://zh.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html#equation-eq-attn-scoring-alpha) 所定义的加性注意力打分函数计算的。

![image-20250505153722147](./assets/image-20250505153722147.png)

## 10.4.2. 定义注意力解码器

首先，初始化解码器的状态，需要下面的输入：

1. 编码器在所有时间步的最终层隐状态，将作为注意力的键和值；
2. 上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；
3. 编码器有效长度（排除在注意力池中填充词元）。

在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。 因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。

## 10.4.3. 训练

## 10.4.4. 小结

- 在预测词元时，如果不是所有输入词元都是相关的，那么具有Bahdanau注意力的循环神经网络编码器-解码器会有选择地统计输入序列的不同部分。这是通过将上下文变量视为加性注意力池化的输出来实现的。
- 在循环神经网络编码器-解码器中，Bahdanau注意力将上一时间步的解码器隐状态视为查询，在所有时间步的编码器隐状态同时视为键和值。

# 10.5. 多头注意力















































