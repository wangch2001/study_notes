# 11.优化算法

优化算法对于深度学习非常重要。一方面，训练复杂的深度学习模型可能需要数小时、几天甚至数周。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原则及其超参数的作用将使我们能够以有针对性的方式调整超参数，以提高深度学习模型的性能。

深度学习中出现的几乎所有优化问题都是*非凸*的。尽管如此，在*凸问题*背景下设计和分析算法是非常有启发性的。

# 11.1. 优化和深度学习

对于深度学习问题，我们通常会先定义*损失函数*。一旦我们有了损失函数，我们就可以使用优化算法来尝试最小化损失。在优化中，损失函数通常被称为优化问题的*目标函数*。

## 11.1.1. 优化的目标

训练误差和泛化误差通常不同：由于优化算法的目标函数通常是基于训练数据集的损失函数，因此优化的目标是减少训练误差。但是，深度学习（或更广义地说，统计推断）的目标是减少泛化误差。

**经验风险**是训练数据集的平均损失，而**风险**则是整个数据群的预期损失。

## 11.1.2. 深度学习中的优化挑战

本章将关注优化算法在最小化目标函数方面的性能，而不是模型的泛化误差。

### 11.1.2.1. 局部最小值

对于任何目标函数f(x)，如果在x处对应的f(x)值小于在x附近任意其他点的f(x)值，那么f(x)可能是局部最小值。如果f(x)在x处的值是整个域中目标函数的最小值，那么f(x)是全局最小值。

深度学习模型的目标函数通常有许多局部最优解。当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数*局部*最优，而不是*全局*最优。只有一定程度的噪声可能会使参数跳出局部最小值。事实上，这是**小批量随机梯度下降**的有利特性之一。在这种情况下，小批量上梯度的自然变化能够将参数从局部极小值中跳出。

### 11.1.2.2. 鞍点

除了局部最小值之外，鞍点是梯度消失的另一个原因。***鞍点*（saddle point）**是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。

![image-20250518131547462](./assets/image-20250518131547462.png)

我们假设函数的输入是k维向量，其输出是标量，因此其Hessian矩阵（也称黑塞矩阵）将有k个特征值（参考[特征分解的在线附录](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html))。函数的解可能是局部最小值、局部最大值或函数梯度为零位置处的鞍点：

- 当函数在零梯度位置处的Hessian矩阵的特征值全部为正值时，我们有该函数的局部最小值；
- 当函数在零梯度位置处的Hessian矩阵的特征值全部为负值时，我们有该函数的局部最大值；
- 当函数在零梯度位置处的Hessian矩阵的特征值为负值和正值时，我们有该函数的一个鞍点。

### 11.1.2.3. 梯度消失

可能遇到的最隐蔽问题是梯度消失。

事实证明，这是在引入ReLU激活函数之前训练深度学习模型相当棘手的原因之一。

## 11.1.3. 小结

- 最小化训练误差并*不能*保证我们找到最佳的参数集来最小化泛化误差。
- 优化问题可能有许多局部最小值。
- 一个问题可能有很多的鞍点，因为问题通常不是凸的。
- 梯度消失可能会导致优化停滞，重参数化通常会有所帮助。对参数进行良好的初始化也可能是有益的。

# 11.2. 凸性

*凸性*（convexity）在优化算法的设计中起到至关重要的作用， 这主要是由于在这种情况下对算法进行分析和测试要容易。

 此外，即使深度学习中的优化问题通常是非凸的， 它们也经常在局部极小值附近表现出一些凸性。

## 11.2.1. 定义

在进行凸分析之前，我们需要定义*凸集*（convex sets）和*凸函数*（convex functions）。

### 11.2.1.1. 凸集

*凸集*（convex set）是凸性的基础。（最优化课程学过）

凸集的并集不一定是凸的，即*非凸*（nonconvex）的。

![image-20250518132027691](./assets/image-20250518132027691.png)

通常，深度学习中的问题是在凸集上定义的。 例如， $\mathbb{R}^d$ ，即实数的d-维向量的集合是凸集（毕竟Rd中任意两点之间的线存在Rd）中。 

### 11.2.1.2. 凸函数

现在我们有了凸集，我们可以引入*凸函数*（convex function）f。

给定一个凸集X，如果对于所有x,x′∈X和所有λ∈[0,1]，函数f:X→R是凸的，我们可以得到


$$
\lambda f(x)+(1-\lambda)f(x^{\prime})\geq f(\lambda x+(1-\lambda)x^{\prime}).
$$


### 11.2.1.3. 詹森不等式

给定一个凸函数f，最有用的数学工具之一就是*詹森不等式*（Jensen’s inequality）。 它是凸性定义的一种推广：


$$
\sum_i\alpha_if(x_i)\geq f\left(\sum_i\alpha_ix_i\right)\mathrm{~and~}E_X[f(X)]\geq f\left(E_X[X]\right),
$$




## 11.2.2. 性质

### 11.2.2.1. 局部极小值是全局极小值

首先凸函数的局部极小值也是全局极小值。可以用反证法给出证明

### 11.2.2.2. 凸函数的下水平集是凸的

我们可以方便地通过凸函数的*下水平集*（below sets）定义凸集。 具体来说，给定一个定义在凸集X上的凸函数f，其任意一个下水平集是凸的。


$$
\mathcal{S}_b:=\{x|x\in\mathcal{X}\mathrm{~and~}f(x)\leq b\}
$$


### 11.2.2.3. 凸性和二阶导数

当一个函数的二阶导数f:Rn→R存在时，我们很容易检查这个函数的凸性。 

## 11.2.3. 约束

凸优化的一个很好的特性是能够让我们有效地处理*约束*（constraints）。 即它使我们能够解决以下形式的*约束优化*（constrained optimization）问题：


$$
\begin{aligned}&\mathrm{minimize~}f(\mathbf{x})\\&\text{subject to }c_i(\mathbf{x})\leq0\text{ for all }i\in\{1,\ldots,N\}.\end{aligned}
$$


这里f是目标函数，ci是约束函数。

### 11.2.3.1. 拉格朗日函数

### 11.2.3.2. 惩罚

### 11.2.3.3. 投影

满足约束条件的另一种策略是*投影*（projections）。

## 11.2.4. 小结

在深度学习的背景下，凸函数的主要目的是帮助我们详细了解优化算法。 我们由此得出梯度下降法和随机梯度下降法是如何相应推导出来的。

- 凸集的交点是凸的，并集不是。
- 根据詹森不等式，“一个多变量凸函数的总期望值”大于或等于“用每个变量的期望值计算这个函数的总值“。
- 一个二次可微函数是凸函数，当且仅当其Hessian（二阶导数矩阵）是半正定的。
- 凸约束可以通过拉格朗日函数来添加。在实践中，只需在目标函数中加上一个惩罚就可以了。
- 投影映射到凸集中最接近原始点的点。

# 11.3. 梯度下降

*预处理*（preconditioning）是梯度下降中的一种常用技术， 还被沿用到更高级的算法中。

## 11.3.1. 一维梯度下降



### 11.3.1.1. 学习率

*学习率*（learning rate）决定目标函数能否收敛到局部最小值，以及何时收敛到最小值。 

如果我们使用的学习率太小，将导致x的更新非常缓慢，需要更多的迭代。 

相反，如果我们使用过高的学习率，x的迭代不能保证降低f(x)的值

### 11.3.1.2. 局部最小值

## 11.3.2. 多元梯度下降


$$
\nabla f(\mathbf{x})=\left[\frac{\partial f(\mathbf{x})}{\partial x_1},\frac{\partial f(\mathbf{x})}{\partial x_2},\ldots,\frac{\partial f(\mathbf{x})}{\partial x_d}\right]^\top.
$$


## 11.3.3. 自适应方法

选择“恰到好处”的学习率η是很棘手的。 如果我们把它选得太小，就没有什么进展；如果太大，得到的解就会振荡，甚至可能发散。 如果我们可以自动确定η，或者完全不必选择学习率，会怎么样？ 

### 11.3.3.1. 牛顿法

### 11.3.3.2. 收敛性分析

### 11.3.3.3. 预处理

计算和存储完整的Hessian非常昂贵，而改善这个问题的一种方法是“预处理”。

### 11.3.3.4. 梯度下降和线搜索

梯度下降的一个关键问题是我们可能会超过目标或进展不足， 解决这一问题的简单方法是结合使用线搜索和梯度下降。

## 11.3.4. 小结

- 学习率的大小很重要：学习率太大会使模型发散，学习率太小会没有进展。
- 梯度下降会可能陷入局部极小值，而得不到全局最小值。
- 在高维模型中，调整学习率是很复杂的。
- 预处理有助于调节比例。
- 牛顿法在凸问题中一旦开始正常工作，速度就会快得多。
- 对于非凸问题，不要不作任何调整就使用牛顿法。

# 11.4. 随机梯度下降

本节继续更详细地说明*随机梯度下降*（stochastic gradient descent）。

## 11.4.1. 随机梯度更新



























