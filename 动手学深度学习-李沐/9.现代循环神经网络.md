# 9.现代循环神经网络

我们将引入两个广泛使用的网络， 即*门控循环单元*（gated recurrent units，GRU）和 *长短期记忆网络*（long short-term memory，LSTM）。然后，我们将基于一个单向隐藏层来扩展循环神经网络架构。 我们将描述具有多个隐藏层的深层架构， 并讨论基于前向和后向循环计算的双向设计。 

# 9.1. 门控循环单元（GRU）

## 9.1.1. 门控隐状态

门控循环单元与普通的循环神经网络之间的关键区别在于： 前者支持隐状态的门控。这意味着模型有专门的机制来确定应该何时更新隐状态， 以及应该何时重置隐状态。

### 9.1.1.1. 重置门和更新门

我们首先介绍*重置门*（reset gate）和*更新门*（update gate）

重置门允许我们控制“可能还想记住”的过去状态的数量； 更新门将允许我们控制新状态中有多少个是旧状态的副本。

### 9.1.1.2. 候选隐状态

应用重置门之后的计算流程。

![image-20250504172242286](./assets/image-20250504172242286.png)

### 9.1.1.3. 隐状态

 这就得出了门控循环单元的最终更新公式：

![image-20250504172335112](./assets/image-20250504172335112.png)

每当更新门Zt接近1时，模型就倾向只保留旧状态。此时，来自Xt的信息基本上被忽略， 从而有效地跳过了依赖链条中的时间步t。 

更新门起作用后的计算流。

![image-20250504172459356](./assets/image-20250504172459356.png)

总之，门控循环单元具有以下两个显著特征：

- 重置门有助于捕获序列中的短期依赖关系；
- 更新门有助于捕获序列中的长期依赖关系。

## 9.1.4. 小结

- 门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。
- 重置门有助于捕获序列中的短期依赖关系。
- 更新门有助于捕获序列中的长期依赖关系。
- 重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。

# 9.2. 长短期记忆网络（LSTM）

长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。 解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）

## 9.2.1. 门控记忆元







































